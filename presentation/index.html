<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reveal.js Presentation with Markdown and Step-by-Step Code Highlights</title>

    <!-- Core Reveal.js styles -->
    <link rel="stylesheet" href="node_modules/reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="node_modules/reveal.js/dist/theme/white.css">

    <!-- Highlight.js styles for syntax highlighting -->
    <link rel="stylesheet" href="node_modules/highlight.js/styles/default.css">
</head>
<body>
<div class="reveal">
<div class="slides">
<section data-markdown>
<textarea data-template>
# Building AI Agents with Go
## A Practical Guide
<img src="https://golang.org/doc/gopher/frontpage.png" alt="Gopher" width="100">
<footer>by Yoni Davidson</footer>
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## About me!
- Married + 1 Daughter and 4 dogs
<hr/>
- **Squad Manager @Tabnine** (Current)
- Founding Engineer At Ariga
- Data Architect / Tech lead at Bond
- SW Engineer at Sears Israel
- SW Engineer at Eyesgiht mobile
- SW Engineer at Alvarion

Note:
* I have large experience in dev tools, AI applications and data engineering.
* Living in Pardes Chana like many known gophers such as Miki (not far from me).    
* I manage the remote squad at Tabnine - which is a cool twist for working hybried 
* At Tabnine we build AI applications for example
Chat, Agents, RAG (client and server)
* Our main languages are typescript , Python and Rust - but I dream at Go at night (and sometimes during the day)
* This talk is about building AI agents with Go, which notes for building AI applications if we were gophers.
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Agenda
- State of AI (why wasn’t this talk done last year?)
- How do we 'talk' with LLMs
- Prompt
- RAG
- One shot agents
- Workflow agents

Note: 
1. Agents are getting mainstream
2. For example, Replit just annonced
3. This talk shows code, the idea is to see that there is no magic in anything, after the talk just fork and use this infra to build your own agents.
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## State of AI
- Pricing
- Availability
- Privacy      
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## State of AI - Pricing 
######  <div style="text-align: left;"> 1M Tokens roughly the equivalent of 2500 pages in a standard book</div>
| Model            | Price per 1M input tokens | Price per 1M output tokens |
|------------------|---------------------------|----------------------------|            
| GPT-4o mini      | $0.15                     | $0.6                       |
| Gemini 1.5 Flash | $0.075                    | $0.30                      |            
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## State of AI - Availability
| Cloud Provider | Claude 3.5 | GPT-4o-mini | LLaMA 3.1 | Gemini 1.5 Flash  | Mistral  |
|----------------|------------|-------------|-----------|-------------------|----------|
| Azure          |            |   ✓         |     ✓     |                   |    ✓     |
| AWS            |     ✓      |             |     ✓     |                   |    ✓     |
| Google Cloud   |    ✓       |             |     ✓     |  ✓                |    ✓     |
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## State of AI - Privacy

* Cloud providers provide both SDK based protection accessing your LLM but also ensure that they don't store or train on your data.
* Most AI providers today have "Enterprise" tiers that ensure data protection.
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## How do we 'talk' with LLMs

* The world is split into Providers and AI Models

* For example - Azure and OpenAI are providers for the GPT-4o-mini model
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## How do we 'talk' with LLMs

* OpenAI client - Demo
```go [9-14|15-24|25-31]
package main

import (
	"fmt"
	"github.com/yonidavidson/gopherconil.talk/prompt"
	"github.com/yonidavidson/gopherconil.talk/provider"
)

func main() {
	p, err := provider.NewOpenAIProvider()
	if err != nil {
		fmt.Println("Error:", err)
		return
	}
	messages := []prompt.Message{
		{
			Role:    prompt.RoleSystem,
			Content: "You are a helpful assistant that provides concise and accurate information.",
		},
		{
			Role:    prompt.RoleUser,
			Content: "Translate the following English text to French: 'Hello, how are you",
		},
	}
	r, err := p.ChatCompletion(messages)
	if err != nil {
		fmt.Println("Error:", err)
		return
	}
	fmt.Println(string(r))
}
```
</textarea>
</section>    

<section data-markdown>
<textarea data-template>
## How do we 'talk' with LLMs
```shell
go run ./cmd/talk
```
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## How do we 'talk' with LLMs
```go [1-8|9-22|23-67|]
// ChatCompletion sends a request to the OpenAI API and returns the response as a byte slice.
func (p OpenAIProvider) ChatCompletion(m []prompt.Message) ([]byte, error) {
	// convert from []prompt.Message to []message
	messages := make([]message, len(m))
	for i, m := range m {
		messages[i] = message{
			Role:    string(m.Role),
			Content: m.Content,
		}
	}
	// Define the payload with a system talk
	payload := requestPayload{
		Model:       "gpt-4o-mini-2024-07-18",
		Messages:    messages,
		MaxTokens:   1000,
		Temperature: 0.5,
		TopP:        1.0,
		N:           1,
		Stop:        nil,
	}

	// Marshal the payload into JSON
	payloadBytes, err := json.Marshal(payload)
	if err != nil {
		return nil, err
	}

	// Create the HTTP request
	req, err := http.NewRequest("POST", endpoint, bytes.NewBuffer(payloadBytes))
	if err != nil {
		return nil, err
	}

	// Set the necessary headers
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+p.APIKey)

	// Execute the request
	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return nil, err
	}
	defer func(Body io.ReadCloser) {
		err := Body.Close()
		if err != nil {
			fmt.Println(err)
		}
	}(resp.Body)
	// Read the response body
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	// Check if the request was successful
	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("unexpected status code: %d, body: %s", resp.StatusCode, string(body))
	}
	var responsePayload responsePayload
	if err := json.Unmarshal(body, &responsePayload); err != nil {
		return nil, err
	}

	return []byte(responsePayload.Choices[0].Message.Content), nil
}```
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Prompt

![Prompt Engineering](images/prompt.svg)

Note:
* Prompt engineering is a very important part of developing the LLM side of the agent.
* We need to be able to not only modify it, but create easy way to evaluate, modify based on limitations such as tokesn
limit or if a specific context was provided
* Some of the things we care about to experiment with is Priority (meaning how many tokens from total can they consume) and the lower the priorty the first one to reduce tokens 
* Location in the prompt - being far from the 'user query' usually means being less in the context  - a very large context and in the end user query, maybe some of that context will not really be addressed.
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Prompt
```go 
`<system>{{.SystemPrompt}}</system>
{{.ChatHistory}}
<user>
Context: {{limitTokens .RAGContext (multiply .MaxTokens 0.2)}}
User Query: {{.UserQuery}}</user>`
```

```go [1-4|5-9|17]
func main() {
	maxTokens := 200
	ragContext := "Paris, the capital of France, is a major European city and a global center for art, fashion, gastronomy, and culture. Its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. Beyond such landmarks as the Eiffel Tower and the 12th-century, Gothic Notre-Dame cathedral, the city is known for its cafe culture and designer boutiques along the Rue du Faubourg Saint-Honoré."
	userQuery := "Can you tell me about the history and main attractions of Paris? Also, what`s the best time to visit and are there any local customs I should be aware of?"
	chatHistory := "<user>I`m planning a trip to Europe.</user>\n
  <assistant>That`s exciting! Europe has many wonderful destinations. Do you have any specific countries or cities in mind?</assistant>
  <user>I am thinking about visiting France.</user>\n
  <assistant>France is a great choice! It offers a rich history, beautiful landscapes, and world-renowned cuisine. Are you interested in visiting Paris or exploring other regions as well?</assistant>"
	systemPrompt := "You are a knowledgeable and helpful travel assistant. Provide accurate and concise information about destinations, attractions, local customs, and travel tips. When appropriate, suggest off-the-beaten-path experiences that tourists might not typically know about. Always prioritize the safety and cultural sensitivity of the traveler."
	data := promptData{
		MaxTokens:    float64(maxTokens),
		RAGContext:   ragContext,
		UserQuery:    userQuery,
		ChatHistory:  chatHistory,
		SystemPrompt: systemPrompt,
	}
	m, err := prompt.ParseMessages(promptTemplate, data)
	if err != nil {
		fmt.Printf("Error parsing messages: %v\n", err)
		return
	}
	p, err := provider.NewOpenAIProvider()
	if err != nil {
		fmt.Printf("Error creating OpenAI provider: %v\n", err)
		return
	}
	r, err := p.ChatCompletion(m)
	if err != nil {
		fmt.Printf("Error getting chat completion: %v\n", err)
		return
	}
	fmt.Println("\n\n\n\n" + string(r))
}
```
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Prompt

```go [|27-55|73-82|90-103]
package prompt

import (
	"fmt"
	"regexp"
	"strings"
	"text/template"
)

type (
	// Message represents a message with a role and content.
	Message struct {
		Role    Role
		Content string
	}
	// Role represents the role of a message.
	Role string
)

const (
	RoleSystem    Role = "system"
	RoleUser      Role = "user"
	RoleAssistant Role = "assistant"
)

// ParseMessages transforms the prompt into a slice of messages.
func ParseMessages(input string, data any) ([]Message, error) {
	pt, err := parse(input, data)
	if err != nil {
		return nil, err
	}
	input = string(pt)
	// Validate tags before parsing
	if err := validate(pt); err != nil {
		return nil, err
	}
	var messages []Message

	// Regular expression to match tags and their content
	re := regexp.MustCompile(`<(system|user|assistant)>([\s\S]*?)</(system|user|assistant)>`)

	// Find all matches in the input string
	matches := re.FindAllStringSubmatch(input, -1)

	for _, match := range matches {
		role := Role(match[1])
		content := strings.TrimSpace(match[2])

		message := Message{
			Role:    role,
			Content: content,
		}

		messages = append(messages, message)
	}

	return messages, nil
}

// validate checks if the input string has matching opening and closing tags for each role.
func validate(input []byte) error {
	roles := []string{"system", "user", "assistant"}
	for _, role := range roles {
		openCount := strings.Count(string(input), "<"+role+">")
		closeCount := strings.Count(string(input), "</"+role+">")
		if openCount != closeCount {
			return fmt.Errorf("mismatched tags for role %s: %d opening, %d closing", role, openCount, closeCount)
		}
	}
	return nil
}

func parse(promptTemplate string, data any) ([]byte, error) {
	tmpl, err := template.New("talk").Funcs(template.FuncMap{
		"limitTokens": limitTokens,
		"multiply": func(a, b float64) float64 {
			return a * b
		},
	}).Parse(promptTemplate)
	if err != nil {
		return nil, fmt.Errorf("error parsing template: %v", err)
	}
	var result strings.Builder
	if err := tmpl.Execute(&result, data); err != nil {
		return nil, fmt.Errorf("error executing template: %v", err)
	}
	return []byte(result.String()), nil
}

func limitTokens(s string, maxTokens float64) string {
	const avgTokenLength = 4 // Average token length heuristic
	maxChars := int(maxTokens * avgTokenLength)

	if len(s) <= maxChars {
		return s
	}

	limited := s[:maxChars]
	lastStop := strings.LastIndexAny(limited, ".?,")
	if lastStop != -1 {
		return limited[:lastStop+1]
	}
	return limited
}```

</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Prompt
```shell
go run ./cmd/prompt
```
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## RAG

* knowledgeable
* Reasoning

Note:
* RAG: Retrieval-Augmented Generation
* Provide the ability to extend the knowledge base
* Allows a more focued knowledge
* Any way you have to retreive data works - Data stored in DB, ELK
* A technique that uses LLM for embedding will be presetned
</textarea>
</section>

<section data-markdown>
<textarea data-template>
  ## RAG

  ###### Embedding
  
  ![Embedding](images/embedding.svg)
  
  Note:
  
</textarea>
</section>
  
<section data-markdown>
<textarea data-template>
## RAG

###### Embedding
```go [36-63]
package rag

import (
	"github.com/yonidavidson/gopherconil.talk/provider"
	"math"
	"sort"
)

type (
	// Rag is a Retrieval Augmented Generation (RAG) struct
	Rag struct {
		provider *provider.OpenAIProvider
	}

	// Embedding represents a text embedding
	Embedding struct {
		text   string
		vector []float64
	}

	// scoredEmbedding represents an Embedding with a score
	scoredEmbedding struct {
		Embedding
		score float64
	}
)

// New creates a new Rag struct
func New(provider *provider.OpenAIProvider) *Rag {
	return &Rag{
		provider: provider,
	}
}

// Embed receives a large text and returns a slice embeddings
func (r *Rag) Embed(text string, chunkSize int) ([]Embedding, error) {
	// Split the text into chunks of the specified size
	var chunks []string
	for i := 0; i < len(text); i += chunkSize {
		end := i + chunkSize
		if end > len(text) {
			end = len(text)
		}
		chunks = append(chunks, text[i:end])
	}

	// Get embeddings for each chunk
	vectors, err := r.provider.TextEmbedding(chunks)
	if err != nil {
		return nil, err
	}

	// Create embeddings slice
	result := make([]Embedding, len(chunks))
	for i, chunk := range chunks {
		result[i] = Embedding{
			text:   chunk,
			vector: vectors[i],
		}
	}

	return result, nil
}

// Search receives a query and a slice of embeddings and returns the most relevant embeddings
func (r *Rag) Search(query string, embeddings []Embedding) ([]byte, error) {
	// Get the Embedding for the query
	queryEmbedding, err := r.provider.TextEmbedding([]string{query})
	if err != nil {
		return nil, err
	}

	// Calculate the similarity between the query Embedding and each text Embedding

	var scoredEmbeddings []scoredEmbedding
	for _, emb := range embeddings {
		score := cosineSimilarity(queryEmbedding[0], emb.vector)
		scoredEmbeddings = append(scoredEmbeddings, scoredEmbedding{emb, score})
	}

	// Sort the embeddings by similarity score in descending order
	sort.Slice(scoredEmbeddings, func(i, j int) bool {
		return scoredEmbeddings[i].score > scoredEmbeddings[j].score
	})

	// Convert scored embeddings back to the original embeddings type
	result := make([]Embedding, len(scoredEmbeddings))
	for i, se := range scoredEmbeddings {
		result[i] = se.Embedding
	}

	return []byte(result[0].text), nil
}

// cosineSimilarity calculates the cosine similarity between two vectors
func cosineSimilarity(a, b []float64) float64 {
	var dotProduct, normA, normB float64
	for i := range a {
		dotProduct += a[i] * b[i]
		normA += a[i] * a[i]
		normB += b[i] * b[i]
	}
	return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}
```
</textarea>  
</section>

<section data-markdown>
<textarea data-template>
    ## RAG
  
    ###### Search
    
    ![Search](images/search.svg)
    
    Note:
    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
  ## RAG
  
  ###### Embedding
  ```go [65-93|95-104]
  package rag
  
  import (
    "github.com/yonidavidson/gopherconil.talk/provider"
    "math"
    "sort"
  )
  
  type (
    // Rag is a Retrieval Augmented Generation (RAG) struct
    Rag struct {
      provider *provider.OpenAIProvider
    }
  
    // Embedding represents a text embedding
    Embedding struct {
      text   string
      vector []float64
    }
  
    // scoredEmbedding represents an Embedding with a score
    scoredEmbedding struct {
      Embedding
      score float64
    }
  )
  
  // New creates a new Rag struct
  func New(provider *provider.OpenAIProvider) *Rag {
    return &Rag{
      provider: provider,
    }
  }
  
  // Embed receives a large text and returns a slice embeddings
  func (r *Rag) Embed(text string, chunkSize int) ([]Embedding, error) {
    // Split the text into chunks of the specified size
    var chunks []string
    for i := 0; i < len(text); i += chunkSize {
      end := i + chunkSize
      if end > len(text) {
        end = len(text)
      }
      chunks = append(chunks, text[i:end])
    }
  
    // Get embeddings for each chunk
    vectors, err := r.provider.TextEmbedding(chunks)
    if err != nil {
      return nil, err
    }
  
    // Create embeddings slice
    result := make([]Embedding, len(chunks))
    for i, chunk := range chunks {
      result[i] = Embedding{
        text:   chunk,
        vector: vectors[i],
      }
    }
  
    return result, nil
  }
  
  // Search receives a query and a slice of embeddings and returns the most relevant embeddings
  func (r *Rag) Search(query string, embeddings []Embedding) ([]byte, error) {
    // Get the Embedding for the query
    queryEmbedding, err := r.provider.TextEmbedding([]string{query})
    if err != nil {
      return nil, err
    }
  
    // Calculate the similarity between the query Embedding and each text Embedding
  
    var scoredEmbeddings []scoredEmbedding
    for _, emb := range embeddings {
      score := cosineSimilarity(queryEmbedding[0], emb.vector)
      scoredEmbeddings = append(scoredEmbeddings, scoredEmbedding{emb, score})
    }
  
    // Sort the embeddings by similarity score in descending order
    sort.Slice(scoredEmbeddings, func(i, j int) bool {
      return scoredEmbeddings[i].score > scoredEmbeddings[j].score
    })
  
    // Convert scored embeddings back to the original embeddings type
    result := make([]Embedding, len(scoredEmbeddings))
    for i, se := range scoredEmbeddings {
      result[i] = se.Embedding
    }
  
    return []byte(result[0].text), nil
  }
  
  // cosineSimilarity calculates the cosine similarity between two vectors
  func cosineSimilarity(a, b []float64) float64 {
    var dotProduct, normA, normB float64
    for i := range a {
      dotProduct += a[i] * b[i]
      normA += a[i] * a[i]
      normB += b[i] * b[i]
    }
    return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
  }
  ```
  </textarea>  
  </section>

<!--<section>
<textarea data-markdown=>
### Ollama
</textarea>
</section> -->
</div>
</div>

<!-- Include Reveal.js core library -->
<script src="node_modules/reveal.js/dist/reveal.js"></script>

<!-- Include Highlight.js library -->
<script src="node_modules/highlight.js/lib/index.js"></script>

<!-- Include the Highlight.js plugin for Reveal.js -->
<script src="node_modules/reveal.js/plugin/highlight/highlight.js"></script>

<!-- Include the Markdown plugin for Reveal.js -->
<script src="node_modules/reveal.js/plugin/markdown/markdown.js"></script>

<script src="node_modules/reveal.js/plugin/notes/notes.js"></script>
<script src="socket.io/socket.io.js"></script>
<script src="node_modules/reveal-notes-server/client.js"></script>

<!-- Initialize Reveal.js and Highlight.js -->
<script>
    // Initialize Reveal.js
    Reveal.initialize({
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],        
    });

    // Initialize syntax highlighting
    hljs.highlightAll();
</script>
</body>
</html>
